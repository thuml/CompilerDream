defaults:

  # Train Script
  logdir: /dev/null
  load_logdir: /dev/null
  seed: 0
  task: dmc_walker_walk
  envs: 1
  envs_parallel: none
  render_size: [64, 64]
  atari_grayscale: True
  time_limit: 0
  action_repeat: 1
  steps: 1e8
  log_every: 1e4
  eval_every: 1e5
  eval_eps: 1
  prefill: 10000
  pretrain: 1
  train_every: 5
  train_steps: 1
  expl_until: 0
  replay: {capacity: 2e6, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: True, dreamsmooth: 0.0}
  dataset: {batch: 16, length: 50}
  log_keys_video: ['image']
  log_keys_sum: '^$'
  log_keys_mean: '^$'
  log_keys_max: '^$'
  precision: 16
  jit: True
  stop_steps: 1002000
  save_all_models: False

  max_return_limit: 1e5
  max_reward_limit: 1e5
  no_eval: False

  # Agent
  clip_rewards: tanh
  expl_behavior: greedy
  expl_noise: 0.0
  eval_noise: 0.0
  eval_state_mean: False

  # Intrinsic bonus parameters
  k: 16
  beta: 0.0
  beta_type: abs
  intr_seq_length: 5
  intr_reward_norm: {momentum: 0.99, scale: 1.0, eps: 1e-8, init: 1.0}
  queue_size: 4096
  queue_dim: 128

  # World Model
  grad_heads: [decoder, reward, discount]
  pred_discount: True
  rssm: {ensemble: 1, hidden: 1024, deter: 1024, stoch: 32, discrete: 32, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1, unimix: 0.0}
  encoder_type: plaincnn
  encoder: {
    mlp_keys: '.*', 
    cnn_keys: '.*', 
    act: elu, 
    norm: none, 
    cnn_depth: 48, 
    cnn_kernels: [4, 4, 4, 4], 
    mlp_layers: [400, 400, 400, 400], 
    res_norm: True,
    res_depth: 3,
    res_layers: 2,
  }
  decoder_type: plaincnn
  decoder: {
    mlp_keys: '.*', 
    cnn_keys: '.*', 
    act: elu, 
    norm: none, 
    cnn_depth: 48, 
    cnn_kernels: [5, 5, 6, 6], 
    mlp_layers: [400, 400, 400, 400], 
    res_norm: True, 
    res_depth: 3,
    res_layers: 2,
  }
  reward_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  discount_head: {layers: 4, units: 400, act: elu, norm: none, dist: binary, dropout: 0.0}
  loss_scales: {
    kl: 1.0, 
    reward: 1.0, 
    discount: 1.0, 
    proprio: 1.0,
    image: 1.0,
    autophase: 1.0,
    action_histogram: 1.0,
    instcount: 1.0,
  }
  kl: {free: 0.0, forward: False, balance: 0.8, free_avg: True}
  model_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}

  # Actor Critic
  actor: {layers: 4, units: 400, act: elu, norm: none, dist: auto, min_std: 0.1, unimix: 0.0, temp: 1.0}
  critic: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  actor_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 1e-6}
  critic_opt: {opt: adam, lr: 2e-4, eps: 1e-5, clip: 100, wd: 1e-6}
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  imag_batch: -1
  actor_grad: auto
  actor_grad_mix: 0.1
  actor_ent: 2e-3
  ent_free: 1e4
  slow_target: True
  slow_target_update: 100
  slow_target_fraction: 1
  slow_actor: True
  slow_baseline: True
  reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}
  reward_perc: False
  reward_perc_params: { momentum: 0.99, perclo: 0.05, perchi: 0.95 }

  # Exploration
  expl_intr_scale: 1.0
  expl_extr_scale: 0.0
  expl_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6}
  expl_head: {layers: 4, units: 400, act: elu, norm: none, dist: mse}
  expl_reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}
  disag_target: stoch
  disag_log: False
  disag_models: 10
  disag_offset: 1
  disag_action_cond: True
  expl_model_loss: kl

  # CompilerGym
  compilergym: {
    mix_weight: [1, 1, 1],
    max_step: 45,
    # Obs
    programl: False,
    abs_inst_obs: False,
    rel_inst_obs: False,
    instc_list_obs: False,
    # Reward
    baseline_thre: 1.0,
    leakiness: 1.0,
    step_penalty: 0.0,
    rew_space: IrInstructionCount,
    # Action
    oz_act: False,
    null_act: False,
    act_space: Autophase,
    # Benchmark iteration
    has_shuffled: False
  }
  coreset_mode: False
  coreset_enumerate_test: False
  reduction_style: default
  action_mask: False

  discount_emph: 20.0
  actor_kl_control: 0.0
  advantage_clip: 100.0
  init_weight: -1.0
  advantage_norm: False
  
  enable_test: False
  test_only: False
  test_interval: 10
  test_eps: 324
  test_dataset: 'suball'

  eval_deter: True

  enable_val: False

compilergym:

  task: compilergym_npb
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}
  grad_heads: [decoder, reward]
  replay: {minlen: 1, maxlen: 45, dreamsmooth: 0.6}
  dataset: {batch: 50, length: 50}
  eval_every: 2000
  log_every: 500
  prefill: 500
  pretrain: 100
  train_every: 5
  eval_deter: True

  # Env
  max_return_limit: 5.0
  max_reward_limit: 3.0
  time_limit: 45
  eval_eps: 50
  action_mask: False

  # Agent
  clip_rewards: identity
  expl_noise: 0.05

  # Model
  rssm: {hidden: 1024, deter: 1024, discrete: 32, stoch: 32}
  .*\.norm$: layer

  model_opt.lr: 2e-4
  actor_opt.lr: 4e-5
  critic_opt.lr: 1e-4
  .*\.wd$: 1e-5

  loss_scales: {
    kl: 0.1,
    reward: 100.0,
    discount: 5.0,
    autophase: 100.0,
    action_histogram: 10.0,
    instcount: 1.0,
  }
  kl.free: 1.0
  ent_free: 2.0
  discount_emph: 1.0

compilergym_dv3:

  reward_head.dist: disc
  critic.dist: disc
  loss_scales.reward: 1.0

  # Policy Regularizer
  reward_perc: True
  reward_perc_params: { momentum: 0.99, perclo: 0.05, perchi: 0.95 }

  # Unimix Categoricals
  rssm.unimix: 0.01
  actor.unimix: 0.01

  # Architecture
  .*\.act$: silu
  .*\.norm$: layer

  # Hyperparams
  model_opt.lr: 1e-4
  actor_opt.lr: 3e-5
  critic_opt.lr: 3e-5
  actor_ent: 3e-4

test:
  test_only: True
  test_interval: 1
  enable_test: True

coreset_train:
  compilergym: {
    act_space: 'NoLimit'
  }
  coreset_mode: True
  reduction_style: 'coreset'
  coreset_enumerate_test: True
  test_dataset: 'coreset_nvp_zeroshot'
  enable_test: True
  test_interval: 1
  test_eps: 11
  loss_scales.reward: 100.0 
  # replay.dreamsmooth: 0.0

coreset_test:
  compilergym: {
    act_space: 'NoLimit'
  }
  coreset_mode: True
  reduction_style: 'coreset'
  coreset_enumerate_test: True
  test_dataset: 'coreset_nvp_zeroshot'
  # test_dataset: 'cbench'
  test_eps: 184
  # test_eps: 11
  test_only: True
  test_interval: 1
  enable_test: True
  eval_state_mean: True

cbench_train:
  task: compilergym_cbench22
  test_dataset: 'cbench'
  test_eps: 23

cbench_train_nolimit:
  task: compilergym_cbench22
  test_dataset: 'cbench'
  test_eps: 23
  compilergym: {
    act_space: 'NoLimit'
  }

instcnt_train:
  loss_scales.instcount: 100.0
  compilergym.abs_inst_obs: True

maxstep_200:
  compilergym.max_step: 200
  time_limit: 200
  replay.maxlen: 200
  prefill: 2800